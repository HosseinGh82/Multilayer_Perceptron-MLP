# üß† Simple Multi-Layer Perceptron (MLP) from Scratch

This project demonstrates the implementation of a basic Multi-Layer Perceptron (MLP) neural network from scratch using Python. We will build the essential components of a neural network, including tensors, operations like addition and multiplication, a simple optimizer, and an automatic differentiation mechanism to enable backpropagation. We will then use this MLP model to classify data generated by `make_blobs` from the `sklearn.datasets` library.

## üåü Introduction

The project is divided into several stages:

1. **Defining Tensors**: Building the foundational data structure to store values and gradients.
2. **Operations on Tensors**: Implementing basic operations like addition, multiplication, and activation functions with automatic differentiation.
3. **Building the MLP Architecture**: Creating neurons, layers, and the overall MLP model.
4. **Training the Model**: Implementing the training loop with forward and backward passes.
5. **Evaluating the Model**: Visualizing the loss over epochs and testing the MLP on a synthetic dataset.


## üõ†Ô∏è Implementation Details
1. **Defining Tensors**
The `Tensor` class is the foundational data structure in this project. Each tensor holds a value, a gradient, and the connections (children) in the computational graph. Various operations such as addition, subtraction, multiplication, division, and power are implemented, each with their respective backward functions for gradient computation.

![Capture4](https://github.com/user-attachments/assets/9b5a246d-58f0-475a-9e7b-49c53a6b102b)


3. **Operations on Tensors**
The project implements basic operations (e.g., addition, multiplication) with automatic differentiation, enabling backpropagation. The `backward` method propagates gradients from the output back through the computational graph.

4. **Building the MLP Architecture**
The MLP model is built using three classes:

- **Neuron**: Represents a single neuron with weights and a bias.
- **Layer**: Consists of multiple neurons.
- **MLP**: Combines multiple layers to form the complete neural network.
  
4. **Training the Model**
The training loop includes forward and backward passes, where the model's parameters are updated using a simple Stochastic Gradient Descent (SGD) optimizer. The loss function used is Mean Squared Error (MSE).

5. **Evaluating the Model**
The model's performance is visualized by plotting the loss over epochs, which helps in understanding the training progress.


## üß™ Testing with a Synthetic Dataset
The MLP model is tested on a synthetic dataset generated using make_blobs from sklearn.datasets. The dataset consists of 1000 samples with 3 centers (classes) and 2 features. The model is trained for 100 epochs, and the loss is plotted to visualize its progression over time.

![Capture2](https://github.com/user-attachments/assets/376ef0b0-90f9-41de-9ce3-afe6f199efa6)
![Capture3](https://github.com/user-attachments/assets/0e857175-e3be-478a-b8c2-a368fdf33911)


## üöÄ Usage
To run the project, open the `MLP_from_Scratch.ipynb` notebook in Jupyter Notebook or JupyterLab, and execute the cells sequentially.


## ‚öôÔ∏è Installation

1. Clone the repository:
   
   ```sh
   git clone https://github.com/yourusername/mlp-from-scratch.git
   cd mlp-from-scratch
   ```
   
2. Install the necessary Python packages:

   ```sh
   pip install -r requirements.txt
   ```

3. Ensure you have Graphviz installed:

  - On Ubuntu: sudo apt-get install graphviz
  - On MacOS: brew install graphviz
  - On Windows: Download and install from the official Graphviz site.
